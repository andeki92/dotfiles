name: Zsh Startup Benchmark

on:
  pull_request:
    paths:
      - 'config/zsh/**'
      - 'scripts/benchmark.sh'
      - '.github/workflows/benchmark.yml'
      - 'docs/benchmarks.md'

jobs:
  benchmark-pr:
    name: Benchmark PR Branch
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.ref }}
          fetch-depth: 0
          
      - name: Install required packages
        run: |
          sudo apt-get update
          sudo apt-get install -y zsh bc stow
      
      - name: Setup home environment
        run: |
          # Create clean home directory structure for test
          mkdir -p $HOME/.config $HOME/.local/share $HOME/.cache
          
          # Install the dotfiles using stow
          if [ -d "config/zsh" ]; then
            stow -d ./config -t $HOME zsh
            echo "✅ Applied zsh configuration from PR branch"
          else
            echo "⚠️ No zsh configuration found in PR branch"
            exit 1
          fi
          
      - name: Run benchmark on PR branch
        id: benchmark-pr
        run: |
          # Run benchmark with save option
          ./scripts/benchmark.sh --save
          
          # Extract the median and average from the CI benchmarks section
          PR_MEDIAN=$(grep -A 10 "^## CI Benchmarks" docs/benchmarks.md | grep -m1 -E '\|.*\|.*\|.*\|.*\|.*\|' | grep -v 'Median \| Average' | awk -F '|' '{print $5}' | tr -d ' s')
          PR_AVERAGE=$(grep -A 10 "^## CI Benchmarks" docs/benchmarks.md | grep -m1 -E '\|.*\|.*\|.*\|.*\|.*\|' | grep -v 'Median \| Average' | awk -F '|' '{print $6}' | tr -d ' s')
          echo "PR_MEDIAN=$PR_MEDIAN" > /tmp/pr_times.txt
          echo "PR_AVERAGE=$PR_AVERAGE" >> /tmp/pr_times.txt
      
      - name: Upload PR benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: pr-benchmark
          path: |
            /tmp/pr_times.txt
            docs/benchmarks.md
  
  benchmark-main:
    name: Benchmark Main Branch
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Main Branch
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0
          
      - name: Install required packages
        run: |
          sudo apt-get update
          sudo apt-get install -y zsh bc stow
      
      - name: Setup home environment
        run: |
          # Create clean home directory structure for test
          mkdir -p $HOME/.config $HOME/.local/share $HOME/.cache
          
          # Install the dotfiles using stow
          if [ -d "config/zsh" ]; then
            stow -d ./config -t $HOME zsh
            echo "✅ Applied zsh configuration from main branch"
          else
            echo "⚠️ No zsh configuration found in main branch"
            exit 1
          fi
          
      - name: Run benchmark on main branch
        id: benchmark-main
        run: |
          # Run benchmark
          OUTPUT=$(./scripts/benchmark.sh)
          
          # Extract the median and average from output
          MAIN_MEDIAN=$(echo "$OUTPUT" | grep "Median startup time" | awk '{print $4}' | tr -d 's')
          MAIN_AVERAGE=$(echo "$OUTPUT" | grep "Average startup time" | awk '{print $4}' | tr -d 's')
          echo "MAIN_MEDIAN=$MAIN_MEDIAN" > /tmp/main_times.txt
          echo "MAIN_AVERAGE=$MAIN_AVERAGE" >> /tmp/main_times.txt
          
      - name: Upload main benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: main-benchmark
          path: /tmp/main_times.txt

  compare-and-comment:
    name: Compare Results and Comment
    needs: [benchmark-pr, benchmark-main]
    runs-on: ubuntu-latest
    
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        
      - name: Compare benchmark results
        run: |
          PR_MEDIAN=$(grep "PR_MEDIAN" pr-benchmark/pr_times.txt | cut -d= -f2)
          PR_AVERAGE=$(grep "PR_AVERAGE" pr-benchmark/pr_times.txt | cut -d= -f2)
          MAIN_MEDIAN=$(grep "MAIN_MEDIAN" main-benchmark/main_times.txt | cut -d= -f2)
          MAIN_AVERAGE=$(grep "MAIN_AVERAGE" main-benchmark/main_times.txt | cut -d= -f2)
          
          # Calculate the median difference
          MEDIAN_DIFF=$(echo "scale=3; $PR_MEDIAN - $MAIN_MEDIAN" | bc)
          echo "MEDIAN_DIFF=$MEDIAN_DIFF" >> $GITHUB_ENV
          
          # Determine if the median is faster or slower
          if (( $(echo "$MEDIAN_DIFF < 0" | bc -l) )); then
            echo "FASTER=true" >> $GITHUB_ENV
            echo "MEDIAN_DIFF_ABS=${MEDIAN_DIFF#-}" >> $GITHUB_ENV
          else
            echo "FASTER=false" >> $GITHUB_ENV
            echo "MEDIAN_DIFF_ABS=$MEDIAN_DIFF" >> $GITHUB_ENV
          fi
          
          # Store times for comment
          echo "PR_MEDIAN=$PR_MEDIAN" >> $GITHUB_ENV
          echo "PR_AVERAGE=$PR_AVERAGE" >> $GITHUB_ENV
          echo "MAIN_MEDIAN=$MAIN_MEDIAN" >> $GITHUB_ENV
          echo "MAIN_AVERAGE=$MAIN_AVERAGE" >> $GITHUB_ENV
      
      - name: Comment on PR
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            
            const mainMedian = process.env.MAIN_MEDIAN;
            const mainAvg = process.env.MAIN_AVERAGE;
            const prMedian = process.env.PR_MEDIAN;
            const prAvg = process.env.PR_AVERAGE;
            const medianDiffAbs = process.env.MEDIAN_DIFF_ABS;
            const faster = process.env.FASTER === 'true';
            
            let performanceEmoji = faster ? '🚀' : '⚠️';
            let performanceText = faster ? 
              `This PR is **${medianDiffAbs}s faster** than main (median: ${prMedian}s vs ${mainMedian}s)` :
              `This PR is **${medianDiffAbs}s slower** than main (median: ${prMedian}s vs ${mainMedian}s)`;
            
            // Read the benchmarks.md file to show the history
            let benchmarkHistory = '';
            try {
              const benchmarkFile = fs.readFileSync('pr-benchmark/benchmarks.md', 'utf8');
              const ciSection = benchmarkFile.split('## CI Benchmarks')[1].split('##')[0];
              benchmarkHistory = ciSection;
            } catch (error) {
              benchmarkHistory = '> Error reading benchmark history';
            }
            
            const comment = `## Zsh Startup Benchmark Results\n\n${performanceEmoji} ${performanceText}\n\n### Detailed Results\n\n| | PR | Main |\n|--|--|--|\n| Median | ${prMedian}s | ${mainMedian}s |\n| Average | ${prAvg}s | ${mainAvg}s |\n\n<details>\n<summary>Benchmark History</summary>\n\n${benchmarkHistory}\n</details>\n\n> **Note:** Both configurations were properly installed with stow in clean environments.\n> For the most accurate results, run \`./scripts/benchmark.sh\` locally both before and after your changes.`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            }); 
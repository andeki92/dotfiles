name: Zsh Startup Benchmark

on:
  pull_request:
    paths:
      - 'config/zsh/**'
      - 'scripts/benchmark.sh'
      - '.github/workflows/benchmark.yml'
      - 'docs/benchmarks.md'

# Add workflow permissions
permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  setup:
    name: Setup Environment
    runs-on: ubuntu-latest
    outputs:
      runner-temp: ${{ steps.temp-dir.outputs.temp-dir }}
    
    steps:
      - name: Get Runner Temp Directory
        id: temp-dir
        run: echo "temp-dir=$RUNNER_TEMP" >> $GITHUB_OUTPUT
        
      - name: Setup APT Cache
        id: apt-cache
        uses: actions/cache@v3
        with:
          path: /var/cache/apt/archives
          key: ${{ runner.os }}-apt-cache-${{ hashFiles('**/benchmark.yml') }}
          restore-keys: |
            ${{ runner.os }}-apt-cache-
      
      - name: Install required packages
        run: |
          # Use multiple mirrors to be more resilient
          echo "Updating apt sources to use multiple mirrors"
          sudo sed -i 's/azure.archive.ubuntu.com/archive.ubuntu.com/g' /etc/apt/sources.list
          
          # Add retry logic for apt operations
          for i in {1..3}; do
            echo "Attempt $i: Running apt-get update"
            if sudo apt-get update; then break; fi
            sleep 5
          done
          
          # Install packages with retry
          for i in {1..3}; do
            echo "Attempt $i: Installing packages"
            if sudo apt-get install -y zsh bc stow; then
              echo "✅ Package installation successful"
              break
            fi
            sleep 5
          done
          
          # Verify installations
          which zsh
          which bc
          which stow
          
          # Create a package verification file for cache validation
          cat > /tmp/apt-validation.txt << EOF
          $(dpkg -l zsh bc stow)
          EOF
      
      - name: Upload package validation
        uses: actions/upload-artifact@v4
        with:
          name: apt-validation
          path: /tmp/apt-validation.txt
      
  benchmark-pr:
    name: Benchmark PR Branch
    needs: setup
    runs-on: ubuntu-latest
    
    steps:
      - name: Download package validation
        uses: actions/download-artifact@v4
        with:
          name: apt-validation
          path: /tmp
      
      - name: Verify packages
        run: |
          echo "Verifying installed packages..."
          cat /tmp/apt-validation.txt
      
      - name: Checkout PR Branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.ref }}
          fetch-depth: 0
      
      - name: Setup home environment
        run: |
          # Create clean home directory structure for test
          mkdir -p $HOME/.config $HOME/.local/share $HOME/.cache
          
          # Install the dotfiles using stow
          if [ -d "config/zsh" ]; then
            stow -d ./config -t $HOME zsh
            echo "✅ Applied zsh configuration from PR branch"
          else
            echo "⚠️ No zsh configuration found in PR branch"
            exit 1
          fi
          
      - name: Run benchmark on PR branch
        id: benchmark-pr
        run: |
          # Run benchmark with save option
          ./scripts/benchmark.sh --save
          
          # Extract the median and average from the CI benchmarks section
          PR_MEDIAN=$(grep -A 10 "^## CI Benchmarks" docs/benchmarks.md | grep -m1 -E '\|.*\|.*\|.*\|.*\|.*\|' | grep -v 'Median \| Average' | awk -F '|' '{print $5}' | tr -d ' s')
          PR_AVERAGE=$(grep -A 10 "^## CI Benchmarks" docs/benchmarks.md | grep -m1 -E '\|.*\|.*\|.*\|.*\|.*\|' | grep -v 'Median \| Average' | awk -F '|' '{print $6}' | tr -d ' s')
          
          # Save results to outputs directory
          mkdir -p ${{ needs.setup.outputs.runner-temp }}/pr-results
          echo "PR_MEDIAN=$PR_MEDIAN" > ${{ needs.setup.outputs.runner-temp }}/pr-results/times.txt
          echo "PR_AVERAGE=$PR_AVERAGE" >> ${{ needs.setup.outputs.runner-temp }}/pr-results/times.txt
          cp docs/benchmarks.md ${{ needs.setup.outputs.runner-temp }}/pr-results/
      
      - name: Upload PR benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: pr-benchmark
          path: ${{ needs.setup.outputs.runner-temp }}/pr-results
  
  benchmark-main:
    name: Benchmark Main Branch
    needs: setup
    runs-on: ubuntu-latest
    
    steps:
      - name: Download package validation
        uses: actions/download-artifact@v4
        with:
          name: apt-validation
          path: /tmp
      
      - name: Verify packages
        run: |
          echo "Verifying installed packages..."
          cat /tmp/apt-validation.txt
      
      - name: Checkout Main Branch
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0
      
      - name: Setup home environment
        run: |
          # Create clean home directory structure for test
          mkdir -p $HOME/.config $HOME/.local/share $HOME/.cache
          
          # Install the dotfiles using stow
          if [ -d "config/zsh" ]; then
            stow -d ./config -t $HOME zsh
            echo "✅ Applied zsh configuration from main branch"
          else
            echo "⚠️ No zsh configuration found in main branch"
            exit 1
          fi
          
      - name: Run benchmark on main branch
        id: benchmark-main
        run: |
          # Run benchmark
          OUTPUT=$(./scripts/benchmark.sh)
          
          # Extract the median and average from output
          MAIN_MEDIAN=$(echo "$OUTPUT" | grep "Median startup time" | awk '{print $4}' | tr -d 's')
          MAIN_AVERAGE=$(echo "$OUTPUT" | grep "Average startup time" | awk '{print $4}' | tr -d 's')
          
          # Save results to outputs directory
          mkdir -p ${{ needs.setup.outputs.runner-temp }}/main-results
          echo "MAIN_MEDIAN=$MAIN_MEDIAN" > ${{ needs.setup.outputs.runner-temp }}/main-results/times.txt
          echo "MAIN_AVERAGE=$MAIN_AVERAGE" >> ${{ needs.setup.outputs.runner-temp }}/main-results/times.txt
          
      - name: Upload main benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: main-benchmark
          path: ${{ needs.setup.outputs.runner-temp }}/main-results

  compare-and-comment:
    name: Compare Results and Comment
    needs: [setup, benchmark-pr, benchmark-main]
    runs-on: ubuntu-latest
    
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        
      - name: Compare benchmark results
        run: |
          PR_MEDIAN=$(grep "PR_MEDIAN" pr-benchmark/times.txt | cut -d= -f2)
          PR_AVERAGE=$(grep "PR_AVERAGE" pr-benchmark/times.txt | cut -d= -f2)
          MAIN_MEDIAN=$(grep "MAIN_MEDIAN" main-benchmark/times.txt | cut -d= -f2)
          MAIN_AVERAGE=$(grep "MAIN_AVERAGE" main-benchmark/times.txt | cut -d= -f2)
          
          # Calculate the median difference
          MEDIAN_DIFF=$(echo "scale=3; $PR_MEDIAN - $MAIN_MEDIAN" | bc)
          echo "MEDIAN_DIFF=$MEDIAN_DIFF" >> $GITHUB_ENV
          
          # Determine if the median is faster or slower
          if (( $(echo "$MEDIAN_DIFF < 0" | bc -l) )); then
            echo "FASTER=true" >> $GITHUB_ENV
            echo "MEDIAN_DIFF_ABS=${MEDIAN_DIFF#-}" >> $GITHUB_ENV
          else
            echo "FASTER=false" >> $GITHUB_ENV
            echo "MEDIAN_DIFF_ABS=$MEDIAN_DIFF" >> $GITHUB_ENV
          fi
          
          # Store times for comment
          echo "PR_MEDIAN=$PR_MEDIAN" >> $GITHUB_ENV
          echo "PR_AVERAGE=$PR_AVERAGE" >> $GITHUB_ENV
          echo "MAIN_MEDIAN=$MAIN_MEDIAN" >> $GITHUB_ENV
          echo "MAIN_AVERAGE=$MAIN_AVERAGE" >> $GITHUB_ENV
      
      - name: Comment on PR
        uses: actions/github-script@v7
        with:
          # Use GITHUB_TOKEN by default, or custom token if available
          github-token: ${{ secrets.BENCHMARK_TOKEN || secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            
            const mainMedian = process.env.MAIN_MEDIAN;
            const mainAvg = process.env.MAIN_AVERAGE;
            const prMedian = process.env.PR_MEDIAN;
            const prAvg = process.env.PR_AVERAGE;
            const medianDiffAbs = process.env.MEDIAN_DIFF_ABS;
            const faster = process.env.FASTER === 'true';
            
            let performanceEmoji = faster ? '🚀' : '⚠️';
            let performanceText = faster ? 
              `This PR is **${medianDiffAbs}s faster** than main (median: ${prMedian}s vs ${mainMedian}s)` :
              `This PR is **${medianDiffAbs}s slower** than main (median: ${prMedian}s vs ${mainMedian}s)`;
            
            // Read the benchmarks.md file to show the history
            let benchmarkHistory = '';
            try {
              const benchmarkFile = fs.readFileSync('pr-benchmark/benchmarks.md', 'utf8');
              const ciSection = benchmarkFile.split('## CI Benchmarks')[1].split('##')[0];
              benchmarkHistory = ciSection;
            } catch (error) {
              benchmarkHistory = '> Error reading benchmark history';
            }
            
            const comment = `## Zsh Startup Benchmark Results\n\n${performanceEmoji} ${performanceText}\n\n### Detailed Results\n\n| | PR | Main |\n|--|--|--|\n| Median | ${prMedian}s | ${mainMedian}s |\n| Average | ${prAvg}s | ${mainAvg}s |\n\n<details>\n<summary>Benchmark History</summary>\n\n${benchmarkHistory}\n</details>\n\n> **Note:** Both configurations were properly installed with stow in clean environments.\n> For the most accurate results, run \`./scripts/benchmark.sh\` locally both before and after your changes.`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            }); 